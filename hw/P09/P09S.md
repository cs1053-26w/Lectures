# Lecture 9 Problems - Solutions

1.
    $$
    \frac{ax + by + c}{gx + hy + 1} - x' \\ 
    \frac{dx + ey + f}{gx + hy + 1} - y' \\
    $$

2. 2

3. 1

4. 4


5.1.  $1 - r^s$

5.2.  $(1 - r^s)^K$

5.3.  $1 - (1 - r^s)^K$

5.4. 
    $$
    \begin{align*}
    1 - (1 - r^s)^K &= P\\
      (1 - r^s)^K &= 1 - P\\
    K \log (1 - r^s) &= \log (1-P) \\
    K &= \left\lceil\frac{\log(1-P)}{\log(1-r^s)} \right\rceil
    \end{align*}
    $$

6. It turns out there cannot be more than one "best" line. This is because the sum-of-squared-residuals loss is *convex*. Very roughly speaking, this means that it's "bowl-shaped", in the sense that if you plot the loss as a function of $w$ and $b$, you get a loss surface similar to our linearized Harris error function; this increases in all directions and has only one unique minimum.

7. The same argument as above applies here; instead of a sum of squares, we have a sum of negative-log terms that have the shape shown in the visualizations in class, where a "very wrong" prediction has very high loss and it decreases quickly and crosses 0 at a correct prediction. This gives rise to a similar "bowl-shaped" loss surface.