{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3109b7e-0092-47a3-aca2-288e745731ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Lecture 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0357b50-2962-423e-85f1-80217ec3d134",
   "metadata": {},
   "source": [
    "#### Announcements\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efd636-c09e-4d1f-a828-33e0fcc75686",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Goals\n",
    "* Be able to describe several different ways 3D reconstructions can be represented (mesh, point cloud, signed distance field, voxels, continuous volumetric representations)\n",
    "* Explain why positional encodings are necessary for MLP neural networks in contexts like fitting 2D images or 3D scene representations.\n",
    "* Be prepared to implement NeRF (Project 4).\n",
    "  * Know how to perform volume rendering along camera rays to predict a pixel color from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be4d6e-4673-4d30-87d6-96a042bef57c",
   "metadata": {},
   "source": [
    "#### 3D Representations\n",
    "\n",
    "* SfM: given images, get camera pose and (**sparse**) 3D scene geometry\n",
    "  * Large-scale SfM result examples:\n",
    "    * Ladybug: http://swehrwein.github.io/sfmflex-vis/Ladybug_demo.html\n",
    "    * Venice: https://facultyweb.cs.wwu.edu/~wehrwes/sfmflex-vis/Venice_demo.html\n",
    "* Multiview stereo / 3D reconstruction: given SfM outputs, recover 3D model of the world\n",
    "  * https://www.youtube.com/watch?v=5ceiOd8Yx3g&t=25s\n",
    "  * https://earth.google.com/web/@47.62102506,-122.3493987,55.50286284a,993.8487854d,35y,18.72359613h,64.09030499t,360r/data=OgMKATA\n",
    "* Interesting question: how do you represent your 3D model?\n",
    "\n",
    "\n",
    "##### Brainstorm\n",
    "\n",
    "How would you reconstruct a 3D model of the world, given images, camera poses, and a sparse point cloud of the world?\n",
    "\n",
    "How would you even *represent* a 3D model of the world?\n",
    "\n",
    "These questions are clearly interrelated. Let's Brainstorm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15aebf-c230-4c49-a102-4e6b111332f0",
   "metadata": {},
   "source": [
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30e2c7-ba6f-48fa-a5ae-bc02d9f73679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if (src_path not in sys.path):\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Library imports\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as skim\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# codebase imports\n",
    "import util\n",
    "import filtering\n",
    "import features\n",
    "import geometry\n",
    "import ML\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # nvidia/cuda\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device('mps') # apple\n",
    "else:\n",
    "    device = torch.device('cpu') # no acceleration\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafabefd-1c2a-4c9c-afc5-4e759e66735d",
   "metadata": {},
   "source": [
    "#### 3D Representations - Some ideas\n",
    "\n",
    "Source for some relevant visuals: https://courses.cs.washington.edu/courses/cse455/10wi/lectures/multiview.pdf\n",
    "\n",
    "* Depth maps; multi-camera: depth map fusion\n",
    "* Voxel grids\n",
    "* Point clouds\n",
    "* Patch clouds (surfels)\n",
    "* Polygon mesh\n",
    "* SDF\n",
    "* Neural network!?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63af11-5cc1-484d-8945-7f73e0226521",
   "metadata": {},
   "source": [
    "### Today: Neural Radiance Fields, and other \"Learned\" 3D Scene Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65396a48-bd3e-42d0-977a-241e03ac51aa",
   "metadata": {},
   "source": [
    "#### ML review: 0 to MLP\n",
    "\n",
    "Review by example the anatomy, care, and feeding of an MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3558766-5647-4fe8-9fa5-087a4f41643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "moons = ML.scale_split(sklearn.datasets.make_moons(n_samples=1000, noise=0.1, random_state=0))\n",
    "X, Xva, y, yva, xx, yy = moons\n",
    "ML.plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b129c-2012-4103-9a90-a3ac2d76fd60",
   "metadata": {},
   "source": [
    "**Code tour**:\n",
    "* look at `ML.MLP()`\n",
    "* look at training routine below\n",
    "* flag me down if something's not familiar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b6f49-6f3d-43e4-82fb-0dbdb69158e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, train_iters=1000):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    for i in range(train_iters):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        batch_indices = torch.randint(0, X.shape[0], (1000,))\n",
    "        \n",
    "        batch_X, batch_y = X[batch_indices,:], y[batch_indices]\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = F.mse_loss(batch_y, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def plot_trained_model(model, X, y, xx, yy, encode=lambda x: x):\n",
    "\n",
    "    with torch.no_grad():     \n",
    "        h, w = xx.shape\n",
    "        dense_X = encode(torch.vstack([xx.flatten(), yy.flatten()]).T)\n",
    "        dense_ypred = model(dense_X).reshape((h, w)).flip([0])\n",
    "        plt.gca().imshow(dense_ypred, extent=[xx.min(), xx.max(), yy.min(),yy.max()])\n",
    "        ML.plot_dataset(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f76c7-1bb7-4082-97de-f16b1a831b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ML.MLP(2, 1)\n",
    "model = train(model, X, y, train_iters=1000)\n",
    "\n",
    "plot_trained_model(model, X, y, xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147dd26-dca9-46b2-b25f-6b1ecedb3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ML.make_stripes(500, 4, 0.01)\n",
    "ML.plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b28454-8fd1-480b-8aef-7c573429cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ML.make_stripes(500, 10, 0.00)\n",
    "xx, yy = np.meshgrid(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01))\n",
    "xx = torch.Tensor(xx)\n",
    "yy = torch.Tensor(yy)\n",
    "model = train(ML.MLP(2,1), X, y)\n",
    "\n",
    "plot_trained_model(model, X, y, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75b463-c67e-4bbe-801f-34072f4f1766",
   "metadata": {},
   "source": [
    "**Conclusion**: high-frequency stuff is hard for the MLP to learn!\n",
    "\n",
    "**Question**: We need to go deeper; will more layers fix this?\n",
    "\n",
    "Try MLP_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ecffe-4356-4270-a9eb-8d6b2a1b433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ML.make_stripes(5000, 10, 0.0)\n",
    "xx, yy = np.meshgrid(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01))\n",
    "xx = torch.Tensor(xx)\n",
    "yy = torch.Tensor(yy)\n",
    "model = train(ML.MLP_N(2, 3, 128, 1), X, y, 2000)\n",
    "\n",
    "plot_trained_model(model, X, y, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523ffc9-b83b-4c42-ac3c-d0e087558c53",
   "metadata": {},
   "source": [
    "To a point, but it's going to get expensive...\n",
    "\n",
    "Alternative: \"positional encoding\"\n",
    "\n",
    "Very handwavy intuition: \"smear\" the input signal across more input channels to allow the network to learn high-frequency stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bfd45-0e65-4bff-9de5-a6eff8c18832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = torch.pi\n",
    "\n",
    "X, y = ML.make_stripes(1000, 15, 0.0)\n",
    "\n",
    "\n",
    "# very barebones positional encoding:\n",
    "def positional_encoding(X):\n",
    "    return torch.hstack([\n",
    "        torch.sin(2*pi * X),\n",
    "        torch.sin(4*pi * X),\n",
    "        torch.sin(8*pi * X),\n",
    "        torch.sin(16*pi * X),\n",
    "        torch.cos(2*pi * X),\n",
    "        torch.cos(4*pi * X),\n",
    "        torch.cos(8*pi * X),\n",
    "        torch.cos(16*pi * X)])\n",
    "\n",
    "Xpe = positional_encoding(X)\n",
    "\n",
    "model = train(ML.MLP(Xpe.shape[1], 1), Xpe, y)\n",
    "\n",
    "plot_trained_model(model, X, y, xx, yy, encode=positional_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d0d3a-aec0-4410-989c-7dc045cce96a",
   "metadata": {},
   "source": [
    "#### Neural Radiance Fields\n",
    "\n",
    "Paper with helpful visuals: https://arxiv.org/pdf/2003.08934.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cfa6ee-e96b-40f3-9525-3b1ffad65cb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Representation: continuous volume with color and density\n",
    "\n",
    "* Basic idea: Parameterize a volumetric representation with an MLP\n",
    "\n",
    "![](../data/nerf_overview.png)\n",
    "\n",
    "**Color** and **density** are a function of 3D location and view direction:\n",
    "\n",
    "$$\n",
    "f(x, y, z, \\phi, \\theta) = (r, g, b, \\sigma)\n",
    "$$\n",
    "\n",
    "  * Detail: density is constrained to depend only on location, not direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4e54a-16d8-4d89-8da1-4fd750aaae52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### MLP Architecture:\n",
    "![](../data/nerf_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824dd4e-f421-4a33-a6bd-9f80df124f5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Volume Rendering\n",
    "\n",
    "Given a magic color-density-producing machine, how do you make an image?\n",
    "\n",
    "(notes, and the above architecture overview figure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d57c19-34a1-4586-a9c6-896a3658ef3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### HW Problem 2\n",
    "The somewhat obfusque equation for weighting samples along a volume rendering ray is:\n",
    "$$\n",
    "C(ùê´)=\\int_{t_n}^{t_f}T(t)\\sigma(ùê´(t))ùêú(ùê´(t),ùêù)dt\n",
    "$$\n",
    "in its continuous form, and the discretized quadriture equation is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{C}(ùê´) &= \\sum_{i=1}^N w_i ùêú_i \\\\\n",
    "\t\t\t\t\t &=\\sum_{i=1}^{N}T_i(1-\\text{exp}(-\\sigma_iŒ¥_i))ùêú_i\n",
    "\\end{align*}\n",
    "$$\n",
    "where N is the number of samples, $T_i=\\text{exp}(-\\sum_{j=1}^{i-1}\\sigma_iŒ¥_i)$, and $Œ¥_i=t_{i+1}-t_i$ is the distance between adjacent samples. This boils down to a weighted sum of the colors ($\\mathbf{c}_i$) along the sample ray.\n",
    "\n",
    "To get some intuition for this, let's plug in a simple case and plot the weights. Let's take samples at $t = 1..10$ and assume that the density is 0 except for an constant-density object with density $\\sigma= 0.4$ ranging between $t=4$ and $t=6$ inclusive. Using software of your choice, plug this situation into the above equation to compute the weights $w_{1..10}$, and plot these to show the weights on the 10 different sample points.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Positional Encoding\n",
    "\n",
    "High-frequencies aren't learned well by the naive implementation, so use **positional encoding**\n",
    "\n",
    "$$\n",
    "Œ≥(p)=(\\text{sin}(2^0œÄp), \\text{cos}(2^0œÄp), \\text{sin}(2^1œÄp), \\text{cos}(2^1œÄp), \\cdots, \\text{sin}(2^{L-2}œÄp), \\text{cos}(2^{L-2}œÄp), \\text{sin}(2^{L-1}œÄp), \\text{cos}(2^{L-1}œÄp))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65812545-149d-4d7a-a0f1-d3987967ad28",
   "metadata": {},
   "source": [
    "#### NeRF Extensions:\n",
    "\n",
    "* Unbounded and higher quality - Mip-Nerf360 Unbounded Anti-Aliased Neural Radiance Fields<https://jonbarron.info/mipnerf360/>\n",
    "* Deformable - TiNeuVox: Fast Dynamic Radiance Fields with Time-Aware Neural Voxels <https://jaminfong.cn/tineuvox/>\n",
    "* Shape and lighting: <https://xiuming.info/projects/nerfactor/>\n",
    "* Editable: <https://zju3dv.github.io/sine/>\n",
    "* [and so on...](https://github.com/awesome-NeRF/awesome-NeRF?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51bad3-7882-4a62-b396-75867eb19b60",
   "metadata": {},
   "source": [
    "#### More Recently: Gaussian Splats\n",
    "\n",
    "Look ma, no MLP!\n",
    "\n",
    "Just a giant cloud of 3D Gaussians that are \"learned\" (optimized) to minimize reprojection error!\n",
    "\n",
    "Shiny results: <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>\n",
    "\n",
    "Paper with some helpful visuals: <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_low.pdf>\n",
    "\n",
    "Video with visualizations that help explain the representation and the learning process: <https://www.youtube.com/watch?v=T_kXY43VZnk&t=61s>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf99b7-f59a-4807-a3d8-6774d59eb170",
   "metadata": {},
   "source": [
    "#### More Classical methods:\n",
    "* Poisson surface reconstruction (2006) <https://doc.cgal.org/latest/Poisson_surface_reconstruction_3/index.html>\n",
    "* Patch-based multi-view stereo (2007) (remained a workhorse through circa 2015+) <https://www.di.ens.fr/pmvs/pmvs-1/index.html>\n",
    "* DeepSDF (2019) - neural signed distance functions; ~direct predecessor to NeRF: <https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
