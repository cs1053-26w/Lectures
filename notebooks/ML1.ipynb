{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3968d7cd-014c-4cf1-92b0-12217eb77e3b",
   "metadata": {},
   "source": [
    "# ML1: Minimizing Loss with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955e629-9389-40f2-8739-757ac1ee18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a139a44-6838-42dd-84f1-b74a48bab95d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Goals\n",
    "* Know how to minimize the linear regression MSE loss using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70419e78-aa24-45bd-8c13-caac5d0aabd6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb4c8d-4f30-4880-9d94-44731b18441f",
   "metadata": {},
   "source": [
    "Recall our linear regression model is: $\\hat{y_i} = wx_i + b$, where:\n",
    "* $(x_i, y_i)$ is an (input, output) pair, also known as (feature, label)\n",
    "* $w, b$ are the weight and bias parameters that wish to optimize to find the best model\n",
    "* $\\hat{y_i}$ is the model's predicted label for the datapoing $x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641251c-b729-4568-8306-42494de1fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(w, b, x):\n",
    "    yhat = w * x + b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202b733-6e9d-4f64-8b77-a87d6c74873a",
   "metadata": {},
   "source": [
    "Recall our loss function is the mean squared error loss: $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Plug the model (an expression for $\\hat{y}_i$) into the loss, and we have:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}(y_i - (wx_i + b))^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2f51d-a373-4b0b-af53-378276d2c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, yhat):\n",
    "    return np.mean((y - yhat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420664f-01d1-4e05-af6b-4557eb35cc3b",
   "metadata": {},
   "source": [
    "What does this expression depend on - or, what is it a function of?\n",
    "* The data $\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$, and\n",
    "* The parameters $w, b$\n",
    "\n",
    "Since the data is what it is - it never changes as we try to fit it (thank goodness) - we will write the loss as a function of only what we have control over: the parameters $w, b$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (wx_i + b))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574aaee4-5a5a-4804-9780-3f21a2fa297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_for_model(x, y, w, b):\n",
    "    yhat = model(w, b, x)\n",
    "    loss = mse_loss(y, yhat)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8d7e2-cd86-4820-a319-0c2413aafad8",
   "metadata": {},
   "source": [
    "## Visualizing the loss\n",
    "\n",
    "Since this is a function of two parameters , I can visualize it with a 2D heat map or a 3D surface plot. Let's create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be517926-c101-499c-9160-dcbbffcd354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 20\n",
    "x = np.linspace(0, 5, n_samples)\n",
    "# True relationship: y = 2*x + 1\n",
    "y_true = 2 * x + 1\n",
    "# Add noise\n",
    "noise = np.random.normal(0, 0.8, n_samples)\n",
    "y = y_true + noise\n",
    "\n",
    "print(f\"Generated {n_samples} data points\")\n",
    "print(f\"x range: [{x.min():.2f}, {x.max():.2f}]\")\n",
    "print(f\"y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"\\nTrue parameters: w=2, b=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919116d1-fd29-430b-9d8b-2f2a3231103f",
   "metadata": {},
   "source": [
    "Now, we'll brute-force compute the loss over a dense grid of choices of $(w, b)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a67f6a-3673-4bf5-8007-255b671ad62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of w and b values\n",
    "w_range = np.linspace(0, 4, 50)\n",
    "b_range = np.linspace(-2, 4, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Compute loss for each (w, b) combination\n",
    "Loss = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        w_val = W[i, j]\n",
    "        b_val = B[i, j]\n",
    "        Loss[i, j] = loss_for_model(x, y, w_val, b_val)\n",
    "\n",
    "print(f\"Loss surface computed: shape {Loss.shape}\")\n",
    "print(f\"Loss range: [{Loss.min():.3f}, {Loss.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d3aec-c5cf-425d-a2b6-bc22c75c675a",
   "metadata": {},
   "source": [
    "...and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e6f311-df45-4b2c-b63f-92b24456e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Create 3D surface plot\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the surface\n",
    "surf = ax.plot_surface(W, B, Loss, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "\n",
    "# Mark the true minimum\n",
    "w_true, b_true = 2.0, 1.0\n",
    "loss_true = loss_for_model(x, y, w_true, b_true)\n",
    "ax.scatter([w_true], [b_true], [loss_true], color='red', s=200, marker='*', \n",
    "           label='True Parameters', zorder=5, edgecolor='darkred', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('w (weight)', fontsize=11, labelpad=10)\n",
    "ax.set_ylabel('b (bias)', fontsize=11, labelpad=10)\n",
    "ax.set_zlabel('MSE Loss', fontsize=11, labelpad=10)\n",
    "ax.set_title('Loss Landscape: MSE as a Function of (w, b)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(surf, ax=ax, label='Loss', pad=0.1)\n",
    "\n",
    "# Set viewing angle\n",
    "ax.view_init(elev=25, azim=45)\n",
    "\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The loss surface looks like a bowl with the minimum at the bottom!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4e41b-8c0f-4ce4-a241-c53c4e0719f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create contour plot (top-down view of the surface)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "contour = ax.contourf(W, B, Loss, levels=30, cmap='viridis', alpha=0.8)\n",
    "contours = ax.contour(W, B, Loss, levels=15, colors='white', alpha=0.3, linewidths=0.5)\n",
    "ax.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "# Mark the true minimum\n",
    "ax.scatter([w_true], [b_true], color='red', s=300, marker='*', \n",
    "           label='True Min (w=2, b=1)', zorder=5, edgecolor='darkred', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('w (weight)', fontsize=12)\n",
    "ax.set_ylabel('b (bias)', fontsize=12)\n",
    "ax.set_title('Loss Landscape Contour Plot: Top-Down View', fontsize=14, fontweight='bold')\n",
    "cbar = fig.colorbar(contour, ax=ax, label='MSE Loss')\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Contour lines connect points with the same loss.\")\n",
    "print(\"The minimum is at the center where the contours are tightest.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c331d63-3ffb-4ff3-a368-a48626b7b01c",
   "metadata": {},
   "source": [
    "## How do we find the best $w$ and $b$?\n",
    "\n",
    "Roll down hill. Repeat:\n",
    "1. Find direction of uphill slope\n",
    "2. Go the other way a bit\n",
    "\n",
    "**Whiteboard:**\n",
    "* 1D case: decide whether to go left or right using the **sign of the derivative**\n",
    "* 2D case: decide what direction to go in using the **direction of the gradient**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadf79a-8012-4e1e-9cde-3df93598ae0d",
   "metadata": {},
   "source": [
    "**Homework Problem 4**\n",
    "\n",
    "For a linear regression model under MSE loss, calculate the derivative of the loss with respect to each of the parameters $w$ and $b$. These are partial derivatives, so when differentiating with respect to $w$ we'll just treat $b$ as a constant, and vice versa.\n",
    "\n",
    "Use the chain rule to break down the problem as follows:\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial }{\\partial \\hat{y_i}} \\left[ \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\right] \\cdot \\frac{\\partial}{\\partial w} \\hat{y_i}$ = \n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial }{\\partial \\hat{y_i}} \\left[ \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\right] \\cdot \\frac{\\partial}{\\partial b} \\hat{y_i}$ = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62a957-053c-4b6a-8087-ca712ec35533",
   "metadata": {},
   "source": [
    "After you've finished your calculations, plug them into the following functions. They'll be used in the visualizations below, so you can check if they're working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "fc3a1e46-284f-4e95-88cb-6989c8e47c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_wrt_w(yhat, x, y):\n",
    "    pass # TODO\n",
    "    \n",
    "def grad_wrt_b(yhat, x, y):\n",
    "    pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b48c1-159d-4454-a410-95c87bfcec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure inline plotting for these cells\n",
    "%matplotlib inline\n",
    "\n",
    "# State for gradient descent\n",
    "state = {\n",
    "    \"w\": 0.0,\n",
    "    \"b\": 0.0,\n",
    "    \"lr\": 0.05,\n",
    "    \"history\": [],  # (step, loss, w, b)\n",
    "    \"step\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "def reset_state(w0=0.0, b0=0.0, lr=0.05):\n",
    "    state[\"w\"] = float(w0)\n",
    "    state[\"b\"] = float(b0)\n",
    "    state[\"lr\"] = float(lr)\n",
    "    state[\"history\"] = []\n",
    "    state[\"step\"] = 0\n",
    "    print(f\"State reset: w={state['w']:.3f}, b={state['b']:.3f}, lr={state['lr']}\")\n",
    "\n",
    "\n",
    "def plot_state():\n",
    "    w, b, step = state[\"w\"], state[\"b\"], state[\"step\"]\n",
    "    yhat = model(w, b, x)\n",
    "    loss = mse_loss(y, yhat)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Left: data + current line\n",
    "    ax = axes[0]\n",
    "    ax.scatter(x, y, s=80, alpha=0.8, label=\"data\", color=\"black\")\n",
    "    ax.plot(x, yhat, color=\"tab:blue\", linewidth=2, label=f\"line (w={w:.2f}, b={b:.2f})\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(f\"Current fit (step {step}, loss {loss:.3f})\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Right: loss history\n",
    "    ax = axes[1]\n",
    "    if state[\"history\"]:\n",
    "        steps = [h[0] for h in state[\"history\"]]\n",
    "        losses = [h[1] for h in state[\"history\"]]\n",
    "        ax.plot(steps, losses, marker=\"o\", color=\"tab:orange\")\n",
    "    ax.set_xlabel(\"step\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.set_title(\"Loss vs steps\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def step_gd(n_steps=1):\n",
    "    \"\"\"Take n_steps of gradient descent and plot after stepping.\"\"\"\n",
    "    for _ in range(n_steps):\n",
    "        yhat = model(state['w'], state['b'], x) # calculate wx + b\n",
    "        \n",
    "        grad_w = grad_wrt_w(yhat, x, y)\n",
    "        grad_b = grad_wrt_b(yhat, x, y)\n",
    "        loss = mse_loss(y, yhat)\n",
    "        state[\"w\"] -= state[\"lr\"] * grad_w\n",
    "        state[\"b\"] -= state[\"lr\"] * grad_b\n",
    "        state[\"step\"] += 1\n",
    "        state[\"history\"].append((state[\"step\"], loss, state[\"w\"], state[\"b\"]))\n",
    "    plot_state()\n",
    "    print(f\"After step {state['step']}: w={state['w']:.4f}, b={state['b']:.4f}, loss={loss:.4f}\")\n",
    "\n",
    "\n",
    "# Initialize\n",
    "reset_state(w0=0.0, b0=0.0, lr=0.05)\n",
    "plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec98381-f91a-4279-8bd7-6c1df56a9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to step through the algorithm:\n",
    "step_gd(n_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e14204-bfc0-4b63-aca6-b1d5e788e94f",
   "metadata": {},
   "source": [
    "## More odds and ends (whiteboard, if time):\n",
    "* Effect of learning rate\n",
    "* Stochastic, batch, and minibatch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
